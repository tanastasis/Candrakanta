import jsonfrom pathlib import Pathfrom typing import Any, Dict, List, Optionalimport reimport htmlfrom fastapi import FastAPI, Request, Query, APIRouter, HTTPException, Body, Form as fofrom fastapi.responses import HTMLResponse, FileResponsefrom fastapi.templating import Jinja2Templatesfrom fastapi import HTTPExceptionfrom fastapi.responses import HTMLResponsefrom fastapi.staticfiles import StaticFilesfrom .engine.paradigm_builder import (export_paradigm_verb,                                      export_paradigm_nominal, export_paradigm_p3,                                      sublexeme, lx, dictionary_form, am, Postfixeme,Grammeme,                                      phonol, buildForm, form_to_json, Form, show_form, rules)from urllib.parse import unquote_plusimport sys, pathlibsys.path.append(str(pathlib.Path(__file__).resolve().parents[1]))BASE_DIR = Path(__file__).resolve().parent.parentDATA_FILE = BASE_DIR / "data" / "lexemes.json"app = FastAPI()templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))app.mount("/static", StaticFiles(directory="static"), name="static")_LEXEMES: Dict[str, Dict[str, Any]] = {}_VARIANTS: dict[str, dict] = {}def load_lexemes() -> Dict[str, Dict[str, Any]]:    with open(DATA_FILE, "r", encoding="utf-8") as f:        data = json.load(f)    norm: Dict[str, Dict[str, Any]] = {}    for key, row in data.items():        morphemes_list = row.get("Morphemes", [])        if not isinstance(morphemes_list, list):            morphemes_list = []  # fallback        norm[key] = {            "name": row.get("name") or key,            "dictform": row.get("DictionaryForm") or "",  # убрал or "" после get, если нет — пустая строка            "meaning": row.get("Meaning") or "",            "category": row.get("Category") or "",            "gender": row.get("Gender") or "",            "groups": ";".join(row.get("Groups", [])) if row.get("Groups") else "",            "morphemes": morphemes_list,  # новое поле: список имен морфем        }    return norm@app.get("/morphemes/{morpheme}/lexemes", response_class=HTMLResponse)def morpheme_lexemes_panel(request: Request, morpheme: str):    lexemes_found = [        {            "name": _LEXEMES[k]["name"],            "dictform": _LEXEMES[k]["dictform"],            "meaning": _LEXEMES[k]["meaning"],        }        for k in _LEXEMES        if morpheme in _LEXEMES[k].get("morphemes", [])    ]    # Правила: прямые (морфема в extract_morphemes) + через группы    morph_obj = am.get(morpheme)    if not morph_obj:        rules_found = []    else:        groups_of_morph = getattr(morph_obj, "Groups", []) or []        # Прямые правила с морфемой        rules_direct = [            rid for rid, rule in rules.items()            if morpheme in extract_morphemes(rule)        ]        # Правила через группы        rules_via_groups = set()        for g in groups_of_morph:            for rid, rule in rules.items():                if g in extract_groups(rule):                    rules_via_groups.add(rid)        # Объединяем и сортируем        all_rules = sorted(            list(set(rules_direct + list(rules_via_groups))),            key=parse_rule_id        )        # Формируем список с id и wording (для отображения)        rules_found = [            {                "id": rid,                "wording": getattr(rules[rid], "wording", "—"),            }            for rid in all_rules        ]    return templates.TemplateResponse(        "partials/morpheme_lexemes.html",        {            "request": request,            "morpheme": morpheme,            "lexemes": lexemes_found,            "rules": rules_found,        }    )@app.on_event("startup")def _startup():    global _LEXEMES    _LEXEMES = load_lexemes()@app.get("/", response_class=HTMLResponse)def home(request: Request):    return templates.TemplateResponse(        "introduction.html",        {"request": request, "active": "intro"}    )@app.get("/lexemes/", response_class=HTMLResponse)def lexemes(request: Request, q: str | None = None):    """    Показываем таблицу лексем. Поддерживаем простой поиск по name/meaning (q).    """    rows: List[Dict[str, Any]] = list(_LEXEMES.values())    if q:        ql = q.lower()        rows = [            r for r in rows            if ql in (r["name"] or "").lower() or ql in (r["meaning"] or "").lower() or  ql in (r["dictform"] or "").lower()        ]    rows.sort(key=lambda r: (r["name"] or "").lower())    return templates.TemplateResponse(        "home.html",        {            "request": request,            "lexemes": rows,            "q": q or "",        },    )@app.get("/morphemes/", response_class=HTMLResponse)def morpheme_list(request: Request, q: str | None = None):    """    Страница со всеми морфемами из morphemes.json. Поддерживаем поиск по shape/meaning (q).    """    items = list(am.values())    if q:        ql = q.lower()        items = [            m for m in items            if ql in m.shape.lower() or ql in m.meaning.lower()        ]    # сортируем по "num", приводя к int    def sort_key(m):        try:            return int(m.get("num") or 0)        except:            return 0    items.sort(key=sort_key)    return templates.TemplateResponse(        "morphemes.html",        {            "request": request,            "items": items,            "q": q or "",        },    )ROMAN_MAP = {    "I":1, "II":2, "III":3, "IV":4, "V":5, "VI":6, "VII":7, "VIII":8, "IX":9, "X":10,    "XI":11, "XII":12, "XIII":13, "XIV":14, "XV":15, "XVI":16, "XVII":17, "XVIII":18, "XIX":19, "XX":20,}def parse_rule_id(rid: str):    """    Превращает 'VI.27.13' → (6, 27, 13)    """    try:        roman, a, b = rid.split(".")        return (ROMAN_MAP.get(roman, 9999), int(a), int(b))    except Exception:        # fallback — отправляем в конец        return (9999, 9999, 9999)def extract_morphemes(rule):    """Возвращает множество морфем, упомянутых в ifs/unlesses.       Поддерживает сложные условия со звёздочкой: '*2~Morpheme~GER2—2~Category~weak'.    """    out = set()    all_conditions = list(rule.ifs) + list(rule.unlesses)    for cond in all_conditions:        # 1) Если условие начинается со "*", значит оно содержит два условия        if cond.startswith("*"):            cond = cond[1:]  # убрать звёздочку            # Разрезать по длинному тире "—" (U+2014)            parts = cond.split("—")            for part in parts:                extract_one_condition(part, out)        else:            extract_one_condition(cond, out)    return outdef extract_one_condition(cond, out_set):    """    Разбирает обычное условие типа:    "2~Morpheme~GER2"    "-1~Morpheme~PL"    "1~Morpheme~NOM,ACC"    И если это Morpheme — добавляет в out_set.    """    parts = cond.split("~")    if len(parts) != 3:        return    _, kind, value = parts    if kind != "Morpheme":        return  # нас интересуют только морфемы    # value может быть "PL" или "NOM,ACC"    for v in value.split(","):        v = v.strip()        if v:            out_set.add(v)def extract_groups(rule):    out = set()    all_conditions = list(rule.ifs) + list(rule.unlesses)    for cond in all_conditions:        if cond.startswith("*"):            cond = cond[1:]            parts = cond.split("—")            for part in parts:                extract_one_group(part, out)        else:            extract_one_group(cond, out)    return outdef extract_one_group(cond, out_set):    parts = cond.split("~")    if len(parts) != 3:        return    _, kind, value = parts    if kind != "Group":        return    # Добавляем split по запятой, как в extract_one_condition    for v in value.split(","):        v = v.strip()        if v:            out_set.add(v)def linkify_wording(wording: str, morphemes_used: set, groups_used: set) -> str:    if not wording:        return wording    # Экранируем " в &quot; для безопасного HTML (только в тексте, не в replacements)    wording = wording.replace('"', '&quot;')    # Определяем класс символов для лексем (с _)    lexeme_chars = r"[A-Za-z0-9_`äăāīūṛśṣṭḍñṅṇ]"    # --- 2) Лексемы с подчёркиванием — упрощённый паттерн с \b (первым приоритетом) ---    def repl_lexeme(match):        lex = match.group(0)        # Добавляем title с meaning, если лексема существует (с экранированием)        if lex in _LEXEMES:            meaning = _LEXEMES[lex].get("meaning", "")            if meaning:                escaped_meaning = html.escape(meaning, quote=True)                title = f'title="{escaped_meaning}"'            else:                title = ""        else:            title = ""        return f'<a class="underline text-amber-400" href="/lexeme/{lex}" {title}>{lex}</a>'  # Золотисто-янтарный (amber-400) для лексем    # Паттерн для лексем: слово_слово с границами \b    lexeme_pattern = rf"\b(?:{lexeme_chars})+\_(?:{lexeme_chars})+\b"    wording = re.sub(        lexeme_pattern,        repl_lexeme,        wording,    )    # Protect lexeme links for morpheme sub    placeholder_map = {}    counter = 0    lexeme_a_pattern = r'<a class="underline text-amber-400"[^>]*>.*?</a>'  # Обновлён под новый класс    matches = list(re.finditer(lexeme_a_pattern, wording))    for match in reversed(matches):  # reverse to avoid position shifts        full_match = match.group(0)        counter += 1        placeholder = f"__LEXEME_PLACEHOLDER_{counter}__"        placeholder_map[placeholder] = full_match        start = match.start()        end = match.end()        wording = wording[:start] + placeholder + wording[end:]    # Now sub morpheme on protected wording    morpheme_chars = r"[A-Za-z0-9äă`āīūṛśṣṭḍñṅṇ]"    morpheme_boundary = rf"(?<!(?:{morpheme_chars}))(?:{morpheme_chars})+(?!(?:{morpheme_chars}))"    # --- 1) Морфемы ---    def repl_morpheme(match):        tok = match.group(0)        if tok in morphemes_used:            morph_obj = am.get(tok)            meaning = morph_obj.meaning if morph_obj else "No meaning"            escaped_meaning = html.escape(meaning, quote=True)            return f'<a class="underline text-cyan-400" href="/morphemes#{tok}" title="{escaped_meaning}">{tok}</a>'  # Лунно-голубой (cyan-400) для морфем        return tok    wording = re.sub(        morpheme_boundary,        repl_morpheme,        wording,    )    # Restore placeholders    for placeholder, full_match in placeholder_map.items():        wording = wording.replace(placeholder, full_match)    # --- 3) Группы — только те, что в условиях ---    for grp in groups_used:        pattern = r"\b" + re.escape(grp) + r"\b"        wording = re.sub(            pattern,            (                f'<a href="#" '                f' class="text-indigo-400 underline group-link" '  # Тёмно-фиолетовый (indigo-400) для групп                f' data-group="{grp}" '                f'>{grp}</a>'            ),            wording        )    return wording@app.get("/rules/group/{group}", response_class=HTMLResponse)def rule_group_panel(request: Request, group: str):    # 1) morphemes.json → все морфемы, содержащие группу    morphemes_found = [        m.name        for m in am.values()        if group in (m.Groups or [])    ]    # 2) lexemes → берём и name, и dictform    lexemes_found = [        {            "name": _LEXEMES[k]["name"],            "dictform": _LEXEMES[k]["dictform"],        }        for k in _LEXEMES.keys()        if group in _LEXEMES[k]["groups"]    ]    return templates.TemplateResponse(        "partials/group_panel.html",        {            "request": request,            "group": group,            "morphemes": morphemes_found,            "lexemes": lexemes_found,        }    )@app.get("/rules", response_class=HTMLResponse)def rules_page(request: Request):    items = []    for rid, rule in rules.items():        wording = getattr(rule, "wording", "—")        morphs = extract_morphemes(rule)        groups_allowed = extract_groups(rule)        wording_html = linkify_wording(wording, morphs, groups_allowed)        items.append({            "id": rid,            "wording": wording_html,        })    # сортировка — как у тебя было    items = sorted(items, key=lambda x: parse_rule_id(x["id"]))    return templates.TemplateResponse(        "rules.html",        {"request": request, "rules_list": items, "active": "rules"}    )@app.get("/lexeme/{lexeme_id}")def lexeme_page(request: Request, lexeme_id: str):    base_lex = lx.get(lexeme_id)    if not base_lex:        raise HTTPException(404, "Lexeme not found")    raw = request.query_params.get("sub", "") or request.query_params.get("SUB", "")    raw = unquote_plus(raw).strip().replace("+", ".").replace(" ", ".")    mors = [p for p in raw.split(".") if p]    cur = base_lex    if mors:        try:            cur = sublexeme(cur, mors, am=am)        except KeyError:            raise HTTPException(404, "Lexeme not found")    cat = (getattr(cur, "Category", "") or "").lower()    if cat in ["verb"]:        payload = export_paradigm_verb(cur)        tpl = "lexeme_verb.html"    elif cur.name == "P_3":        payload = export_paradigm_p3(cur)        tpl = "lexeme_nominal.html"    else:        payload = export_paradigm_nominal(cur)        tpl = "lexeme_nominal.html"    for d in payload.get("_derivations", []):        _VARIANTS[d["variant_id"]] = d    return templates.TemplateResponse(        tpl,        {            "request": request,            "payload": payload,            "root_lexeme_id": lexeme_id,            "current_sub_chain": ".".join(mors),        },    )@app.get("/api/variant/{variant_id}/derivation", response_class=HTMLResponse)def variant_derivation_partial(request: Request, variant_id: str):    data = _VARIANTS.get(variant_id)    if not data:        raise HTTPException(status_code=404, detail="Variant not found")    # Формируем base: все морфемы из морфем лексемы    base = []    lexeme_id = data.get("lexeme_id", "")    mors = lexeme_id.split("+")    for m in mors:        if m in _LEXEMES:            lexmors = _LEXEMES[m].get("morphemes", [])            for lexmor in lexmors:                base.append(lexmor)    n = len(base)    # enrich steps with rule wording    steps = data.get("steps", [])    for s in steps:        rid = str(s.get("rule_id", ""))        s["wording"] = getattr(rules.get(rid, None), "wording", "") if rid in rules else ""    # Только для первого step: linkify form (первые n — из base, остальные — из form_str)    if steps:        first_step = steps[0]        form_str = first_step.get("form", "")        form_parts = [p.strip() for p in form_str.split("-") if p.strip()]                if form_parts:            # Первые n: используем base[i] для ссылки, но отображаем как form_parts[i]            # Если n > len(form_parts) — обрезаем, чтобы не вылететь            links = []            for i, part in enumerate(form_parts):                if i < n and i < len(base):                    link_part = base[i]  # оригинальное имя для ссылки                else:                    link_part = part  # как в form                links.append(f'<a class="text-cyan-400 underline" href="/morphemes#{link_part}">{part}</a>')                        first_step["form_html"] = "-".join(links)        else:            first_step["form_html"] = '<span class="text-gray-400">—</span>'    data["steps"] = steps  # update in place    return templates.TemplateResponse(        "partials/derivation_table.html",        {"request": request, "d": data},    )@app.get("/candrakanta.pdf")def view_rules_pdf():    return FileResponse(        "data/candrakanta.pdf",        media_type="application/pdf",        filename="candrakanta.pdf",        headers={"Content-Disposition": 'inline; filename="rules.pdf"'}  # inline    )@app.get("/lexemes.xlsx")def download_lexemes_excel():    return FileResponse(        "data/lexemes.xlsx",        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",        filename="lexemes.xlsx"    )@app.post("/api/variant/{variant_id}/with_postfix", response_class=HTMLResponse)def variant_with_postfix(    request: Request,    variant_id: str,    post: str = fo(None),    show_all: str = fo("0"),):    base = _VARIANTS.get(variant_id)    if not base:        raise HTTPException(404, "Variant not found")    lexeme_id = base.get("lexeme_id")    gr_obj    = base.get("gr_obj")    if not lexeme_id or gr_obj is None:        raise HTTPException(400, "Variant lacks lexeme/grammeme context")    if "+" in lexeme_id:        mors = lexeme_id.split("+")        root = mors[0]        try:            lex = sublexeme(lx.get(root), mors[1:], am=am)        except Exception:            raise HTTPException(404, "Sublexeme not found")    else:        lex = lx.get(lexeme_id)    if not lex:        raise HTTPException(404, "Lexeme not found")    cat = (getattr(lex, "Category", "") or "").lower()    if cat == "verb":        allowed_posts = ["", "EMPH+NOM", "1+SG+OBL", "2+SG+OBL", "3+SG+OBL", "1+PL+OBL", "2+PL+OBL", "3+PL+OBL"]    else:        allowed_posts = ["", "EMPH+NOM"]    if post is not None and post not in allowed_posts:        raise HTTPException(400, f"Postfix '{post}' is not allowed for category '{cat}'")    post_list = []    if post:        for tok in (post.split("+") if post else []):            if tok:                if tok not in am:                    raise HTTPException(422, f"Unknown postfixeme morpheme: {tok}")                post_list.append(am[tok])    post_obj = Postfixeme(post_list) if post_list else None    form_obj = phonol(buildForm(lex, gr_obj, post_obj) if post_obj else buildForm(lex, gr_obj))    grammeme_ids = [getattr(g, "id", None) or getattr(g, "name", None) for g in getattr(gr_obj, "l", [])]    grammeme_ids = [x for x in grammeme_ids if x]  # фильтруем None/''    cell_json, derivs = form_to_json(        lexeme_id,        grammemes=grammeme_ids,        postfixemes=(post.split("+") if post else []),        form_obj=form_obj,        cell_label=None,    )    for d_ in derivs:        d_["lexeme_id"] = lexeme_id        d_["gr_obj"]    = gr_obj        for s in d_.get("steps", []):            rid = str(s.get("rule_id", "")); obj = rules.get(rid)            s["wording"] = getattr(obj, "wording", "") if obj else ""        _VARIANTS[d_["variant_id"]] = d_    surface = base.get("surface", "")    if cell_json["variants"]:        surface = next((v["surface"] for v in cell_json["variants"] if v["is_canonical"]),                       cell_json["variants"][0]["surface"])    labels = {"": "SIMPLE", "EMPH+NOM": "EMPH+NOM", "1+SG+OBL": "1+SG+OBL", "2+SG+OBL": "2+SG+OBL", "3+SG+OBL": "3+SG+OBL",              "1+PL+OBL": "1+PL+OBL", "2+PL+OBL": "2+PL+OBL", "3+PL+OBL": "3+PL+OBL"}    d = {        "variant_id": variant_id,        "surface": surface,        "steps": [],        "variants": cell_json["variants"],        "allowed_posts": allowed_posts,        "labels": labels,    }    if post == "":        d["initial_variant_id"] = variant_id    else:        canon = [v for v in cell_json["variants"] if v["is_canonical"]]        d["initial_variant_id"] = (canon[0]["variant_id"] if canon else (cell_json["variants"][0]["variant_id"] if cell_json["variants"] else None))    return templates.TemplateResponse(        "partials/postfix_block.html",        {"request": request, "d": d, "show_all": show_all},    )